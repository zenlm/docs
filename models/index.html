<!DOCTYPE html><!--2ezz_PBbsRcymwbmfjjIq--><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="stylesheet" href="/_next/static/css/1d1f6bc532e5f43f.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-2eb758dea75faf50.js"/><script src="/_next/static/chunks/4bd1b696-c023c6e3521b1417.js" async=""></script><script src="/_next/static/chunks/255-bc8b3b9a93d25a64.js" async=""></script><script src="/_next/static/chunks/main-app-06c948c55f93babd.js" async=""></script><script src="/_next/static/chunks/619-ba102abea3e3d0e4.js" async=""></script><script src="/_next/static/chunks/394-7ae625d7fca91f5c.js" async=""></script><script src="/_next/static/chunks/app/layout-69b551721b5ae1f6.js" async=""></script><script src="/_next/static/chunks/app/page-6f8a3b3ea33cc66d.js" async=""></script><script src="/assets/js/main.js" async=""></script><link rel="icon" type="image/png" href="/favicon.png"/><title>Zen LM Models - Complete Model Family</title><meta name="description" content="Complete collection of Zen Language Models - from nano to next-gen, language to multimodal"/><meta name="keywords" content="AI, LLM, Agentic AI, Code Generation, Zen Coder, Multimodal, Open Source, Machine Learning"/><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body><div hidden=""><!--$--><!--/$--></div><header><nav class="navbar"><div class="container"><div class="logo"><a href="/"><img alt="Zen LM" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="logo-img" style="color:transparent" src="/logo.png"/><div class="logo-text"><h1>Zen LM</h1></div></a></div><div class="header-right"><div class="nav-links"><a class="" href="/">Home</a><a class="active" href="/models/">Models</a><a class="" href="/datasets/">Datasets</a><a class="" href="/research/">Research</a></div><div class="logo-links"><a href="https://github.com/zenlm" target="_blank" rel="noopener noreferrer" class="icon-link" title="GitHub"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg></a><a href="https://huggingface.co/zenlm" target="_blank" rel="noopener noreferrer" class="icon-link" title="HuggingFace"><img alt="HuggingFace" loading="lazy" width="28" height="28" decoding="async" data-nimg="1" style="color:transparent" src="/hf-logo.png"/></a></div></div></div></nav></header><main><section class="hero"><div class="container"><h2 class="hero-title">Zen Model Family</h2><p class="hero-subtitle">24+ models spanning language, vision, audio, video, 3D, and specialized tasks</p><p class="hero-description">Complete model collection from 0.6B to 1T+ parameters. From efficient edge deployment to powerful cloud inference, each model is optimized for specific use cases while maintaining the same high standards of performance, transparency, and open-source accessibility.</p></div></section><section id="core-models" class="models-section"><div class="container"><h2 class="section-title">Core Language Models</h2><p class="section-subtitle">Foundational models from nano to next-gen</p><div class="models-grid"><div class="model-card complete"><div class="model-header"><h3>zen-nano</h3><span class="badge badge-complete">Available</span></div><div class="model-specs"><div class="spec"><span class="spec-label">Parameters</span><span class="spec-value">0.6B</span></div><div class="spec"><span class="spec-label">Base</span><span class="spec-value">Qwen3-0.6B</span></div><div class="spec"><span class="spec-label">Context</span><span class="spec-value">32K tokens</span></div><div class="spec"><span class="spec-label">Architecture</span><span class="spec-value">28 layers, GQA</span></div></div><p class="model-description">Ultra-efficient model for edge deployment and embedded systems. Perfect for on-device AI applications with minimal resource requirements.</p><div class="model-formats"><span class="format-tag">SafeTensors</span><span class="format-tag">GGUF</span><span class="format-tag">MLX</span></div><div class="model-actions"><a href="https://huggingface.co/zenlm/zen-nano" class="btn btn-sm btn-primary" target="_blank" rel="noopener noreferrer">ðŸ¤— HF</a><a href="https://github.com/zenlm/zen-nano" class="btn btn-sm btn-secondary" target="_blank" rel="noopener noreferrer">ðŸ“¦ GitHub</a><a href="https://github.com/zenlm/zen-nano/tree/main/docs" class="btn btn-sm btn-secondary" target="_blank" rel="noopener noreferrer">ðŸ“– Docs</a><a href="/papers/zen-nano_whitepaper.pdf" class="btn btn-sm btn-secondary" target="_blank" rel="noopener noreferrer">ðŸ“„ Paper</a></div></div><div class="model-card complete"><div class="model-header"><h3>zen-eco</h3><span class="badge badge-complete">Available</span></div><div class="model-specs"><div class="spec"><span class="spec-label">Parameters</span><span class="spec-value">4B</span></div><div class="spec"><span class="spec-label">Base</span><span class="spec-value">Qwen3-3B</span></div><div class="spec"><span class="spec-label">Context</span><span class="spec-value">32K tokens</span></div><div class="spec"><span class="spec-label">Variants</span><span class="spec-value">Instruct, Agent, Coder, Thinking</span></div></div><p class="model-description">Balanced performance and efficiency for general-purpose applications. Multiple specialized variants for different use cases.</p><div class="model-formats"><span class="format-tag">SafeTensors</span><span class="format-tag">GGUF</span><span class="format-tag">MLX</span></div><div class="model-actions"><a href="https://huggingface.co/zenlm/zen-eco" class="btn btn-sm btn-primary" target="_blank" rel="noopener noreferrer">ðŸ¤— HF</a><a href="https://github.com/zenlm/zen-eco" class="btn btn-sm btn-secondary" target="_blank" rel="noopener noreferrer">ðŸ“¦ GitHub</a></div></div><div class="model-card complete"><div class="model-header"><h3>zen-omni</h3><span class="badge badge-complete">Available</span></div><div class="model-specs"><div class="spec"><span class="spec-label">Parameters</span><span class="spec-value">7B</span></div><div class="spec"><span class="spec-label">Base</span><span class="spec-value">Qwen3-Omni</span></div><div class="spec"><span class="spec-label">Modalities</span><span class="spec-value">Text + Vision + Audio</span></div><div class="spec"><span class="spec-label">Type</span><span class="spec-value">Multimodal</span></div></div><p class="model-description">Multimodal model based on Qwen3-Omni supporting text, vision, and audio understanding simultaneously. NOT Qwen2.5!</p><div class="model-formats"><span class="format-tag">SafeTensors</span></div><div class="model-actions"><a href="https://huggingface.co/zenlm/zen-omni" class="btn btn-sm btn-primary" target="_blank" rel="noopener noreferrer">ðŸ¤— HF</a><a href="https://github.com/zenlm/zen-omni" class="btn btn-sm btn-secondary" target="_blank" rel="noopener noreferrer">ðŸ“¦ GitHub</a><a href="https://github.com/zenlm/zen-omni/tree/main/docs" class="btn btn-sm btn-secondary" target="_blank" rel="noopener noreferrer">ðŸ“– Docs</a><a href="/papers/zen-omni_whitepaper.pdf" class="btn btn-sm btn-secondary" target="_blank" rel="noopener noreferrer">ðŸ“„ Paper</a></div></div><div class="model-card complete"><div class="model-header"><h3>zen-coder</h3><span class="badge badge-complete">Available</span></div><div class="model-specs"><div class="spec"><span class="spec-label">Parameters</span><span class="spec-value">14B</span></div><div class="spec"><span class="spec-label">Base</span><span class="spec-value">Qwen3-Coder-14B</span></div><div class="spec"><span class="spec-label">Context</span><span class="spec-value">128K tokens</span></div><div class="spec"><span class="spec-label">Focus</span><span class="spec-value">Code Generation</span></div></div><p class="model-description">Specialized for code generation, debugging, and software engineering tasks. Supports 100+ programming languages with extended context.</p><div class="model-formats"><span class="format-tag">SafeTensors</span><span class="format-tag">GGUF</span><span class="format-tag">MLX</span></div><div class="model-actions"><a href="https://huggingface.co/zenlm/zen-coder" class="btn btn-sm btn-primary" target="_blank" rel="noopener noreferrer">ðŸ¤— HF</a><a href="https://github.com/zenlm/zen-coder" class="btn btn-sm btn-secondary" target="_blank" rel="noopener noreferrer">ðŸ“¦ GitHub</a></div></div><div class="model-card complete"><div class="model-header"><h3>zen-next</h3><span class="badge badge-complete">Available</span></div><div class="model-specs"><div class="spec"><span class="spec-label">Parameters</span><span class="spec-value">32B</span></div><div class="spec"><span class="spec-label">Base</span><span class="spec-value">Qwen3-32B</span></div><div class="spec"><span class="spec-label">Context</span><span class="spec-value">32K tokens</span></div><div class="spec"><span class="spec-label">Focus</span><span class="spec-value">Frontier</span></div></div><p class="model-description">Our flagship model pushing the boundaries of performance and capability. For the most demanding applications requiring maximum intelligence.</p><div class="model-formats"><span class="format-tag">SafeTensors</span><span class="format-tag">GGUF</span></div><div class="model-actions"><a href="https://github.com/zenlm/zen-next" class="btn btn-sm btn-secondary" target="_blank" rel="noopener noreferrer">ðŸ“¦ GitHub</a></div></div></div></div></section><section id="multimodal" class="models-section"><div class="container"><h2 class="section-title">Multimodal Models</h2><p class="section-subtitle">Vision, Audio, Video, and 3D Generation</p><div class="models-grid"><div class="model-card complete"><div class="model-header"><h3>zen-vl</h3><span class="badge badge-complete">Available</span></div><div class="model-specs"><div class="spec"><span class="spec-label">Type</span><span class="spec-value">Vision-Language</span></div><div class="spec"><span class="spec-label">Base</span><span class="spec-value">Qwen3-VL</span></div><div class="spec"><span class="spec-label">Sizes</span><span class="spec-value">4B, 8B, 30B</span></div><div class="spec"><span class="spec-label">Variants</span><span class="spec-value">Instruct, Agent</span></div><div class="spec"><span class="spec-label">Focus</span><span class="spec-value">Function Calling</span></div></div><p class="model-description">Next-generation vision-language model with advanced function calling capabilities. Trained on Agent Data Protocol (ADP) and xLAM datasets for superior agent performance and tool use.</p><div class="model-formats"><span class="format-tag">SafeTensors</span><span class="format-tag">GGUF</span></div><div class="model-actions"><a href="https://huggingface.co/zenlm/zen-vl-4b-instruct" class="btn btn-sm btn-primary" target="_blank" rel="noopener noreferrer">ðŸ¤— HF</a><a href="https://github.com/zenlm/zen-vl" class="btn btn-sm btn-secondary" target="_blank" rel="noopener noreferrer">ðŸ“¦ GitHub</a></div></div><div class="model-card complete"><div class="model-header"><h3>zen-designer</h3><span class="badge badge-complete">Available</span></div><div class="model-specs"><div class="spec"><span class="spec-label">Type</span><span class="spec-value">Vision-Language</span></div><div class="spec"><span class="spec-label">Base</span><span class="spec-value">Qwen-VL</span></div><div class="spec"><span class="spec-label">Variants</span><span class="spec-value">Instruct, Thinking</span></div><div class="spec"><span class="spec-label">Focus</span><span class="spec-value">Visual Understanding</span></div></div><p class="model-description">Advanced vision-language model for image understanding, analysis, and reasoning. Supports visual question answering, OCR, and detailed scene description.</p><div class="model-formats"><span class="format-tag">SafeTensors</span></div><div class="model-actions"><a href="https://github.com/zenlm/zen-designer" class="btn btn-sm btn-secondary" target="_blank" rel="noopener noreferrer">ðŸ“¦ GitHub</a></div></div><div class="model-card complete"><div class="model-header"><h3>zen-artist</h3><span class="badge badge-complete">Available</span></div><div class="model-specs"><div class="spec"><span class="spec-label">Type</span><span class="spec-value">Text-to-Image</span></div><div class="spec"><span class="spec-label">Base</span><span class="spec-value">Qwen-Image</span></div><div class="spec"><span class="spec-label">Variants</span><span class="spec-value">Base, Edit</span></div><div class="spec"><span class="spec-label">Focus</span><span class="spec-value">Image Generation</span></div></div><p class="model-description">High-quality image generation from text descriptions. zen-artist-edit provides advanced image editing capabilities with natural language instructions.</p><div class="model-formats"><span class="format-tag">SafeTensors</span><span class="format-tag">Diffusers</span></div><div class="model-actions"><a href="https://github.com/zenlm/zen-artist" class="btn btn-sm btn-secondary" target="_blank" rel="noopener noreferrer">ðŸ“¦ GitHub</a></div></div><div class="model-card complete"><div class="model-header"><h3>zen-video</h3><span class="badge badge-complete">Available</span></div><div class="model-specs"><div class="spec"><span class="spec-label">Type</span><span class="spec-value">Text-to-Video</span></div><div class="spec"><span class="spec-label">Base</span><span class="spec-value">HunyuanVideo</span></div><div class="spec"><span class="spec-label">Variants</span><span class="spec-value">T2V, I2V</span></div><div class="spec"><span class="spec-label">Focus</span><span class="spec-value">Video Generation</span></div></div><p class="model-description">State-of-the-art video generation from text descriptions. zen-video-i2v provides image-to-video generation with fine control over motion and dynamics.</p><div class="model-formats"><span class="format-tag">SafeTensors</span></div><div class="model-actions"><a href="https://github.com/zenlm/zen-video" class="btn btn-sm btn-secondary" target="_blank" rel="noopener noreferrer">ðŸ“¦ GitHub</a></div></div><div class="model-card complete"><div class="model-header"><h3>zen-3d</h3><span class="badge badge-complete">Available</span></div><div class="model-specs"><div class="spec"><span class="spec-label">Type</span><span class="spec-value">3D Generation</span></div><div class="spec"><span class="spec-label">Input</span><span class="spec-value">Text, Image, Point Cloud</span></div><div class="spec"><span class="spec-label">Output</span><span class="spec-value">3D Meshes</span></div><div class="spec"><span class="spec-label">Focus</span><span class="spec-value">3D Assets</span></div></div><p class="model-description">Generate high-quality 3D models from various input modalities. Perfect for game development, AR/VR, and 3D content creation.</p><div class="model-formats"><span class="format-tag">SafeTensors</span></div><div class="model-actions"><a href="https://github.com/zenlm/zen-3d" class="btn btn-sm btn-secondary" target="_blank" rel="noopener noreferrer">ðŸ“¦ GitHub</a></div></div><div class="model-card complete"><div class="model-header"><h3>zen-musician</h3><span class="badge badge-complete">Available</span></div><div class="model-specs"><div class="spec"><span class="spec-label">Type</span><span class="spec-value">Music Generation</span></div><div class="spec"><span class="spec-label">Input</span><span class="spec-value">Text, Audio</span></div><div class="spec"><span class="spec-label">Output</span><span class="spec-value">Music, Audio</span></div><div class="spec"><span class="spec-label">Focus</span><span class="spec-value">Music Creation</span></div></div><p class="model-description">Generate high-quality music from text descriptions or audio samples. Supports multiple genres, instruments, and musical styles.</p><div class="model-formats"><span class="format-tag">SafeTensors</span></div><div class="model-actions"><a href="https://github.com/zenlm/zen-musician" class="btn btn-sm btn-secondary" target="_blank" rel="noopener noreferrer">ðŸ“¦ GitHub</a></div></div></div></div></section></main><!--$--><!--/$--><footer><div class="container"><div class="footer-content"><div class="footer-section"><h4>Zen LM</h4><p>Open foundation models for agentic AI. 30+ models from 0.6B to 1T parameters.</p></div><div class="footer-section"><h4>Zen Coder</h4><ul><li><a href="/models/#zen-coder">Zen Coder 4B</a></li><li><a href="/models/#zen-coder">Zen Coder 24B</a></li><li><a href="/models/#zen-coder">Zen Coder 123B</a></li><li><a href="/models/#zen-coder">Zen Coder Max (358B)</a></li><li><a href="/models/#zen-coder">Zen Coder Ultra (1T)</a></li></ul></div><div class="footer-section"><h4>Model Family</h4><ul><li><a href="/models/#core-models">zen-nano (0.6B)</a></li><li><a href="/models/#core-models">zen-eco (4B)</a></li><li><a href="/models/#core-models">zen-omni (7B)</a></li><li><a href="/models/#multimodal">zen-vl (Vision)</a></li><li><a href="/models/#multimodal">zen-3d (3D Gen)</a></li></ul></div><div class="footer-section"><h4>Resources</h4><ul><li><a href="/datasets/">Training Data</a></li><li><a href="https://huggingface.co/zenlm" target="_blank" rel="noopener noreferrer">HuggingFace</a></li><li><a href="https://github.com/zenlm" target="_blank" rel="noopener noreferrer">GitHub</a></li><li><a href="/research/">Research Papers</a></li><li><a href="https://github.com/zenlm/zen-trainer" target="_blank" rel="noopener noreferrer">zen-trainer</a></li></ul></div></div><div class="footer-bottom"><p>Â© 2025 Zen Authors. All rights reserved. Built with clarity and purpose.</p></div></div></footer><script src="/_next/static/chunks/webpack-2eb758dea75faf50.js" id="_R_" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[1398,[\"619\",\"static/chunks/619-ba102abea3e3d0e4.js\",\"394\",\"static/chunks/394-7ae625d7fca91f5c.js\",\"177\",\"static/chunks/app/layout-69b551721b5ae1f6.js\"],\"default\"]\n3:I[9766,[],\"\"]\n4:I[8924,[],\"\"]\n5:I[2619,[\"619\",\"static/chunks/619-ba102abea3e3d0e4.js\",\"974\",\"static/chunks/app/page-6f8a3b3ea33cc66d.js\"],\"\"]\nd:I[7150,[],\"\"]\n:HL[\"/_next/static/css/1d1f6bc532e5f43f.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"2ezz-PBbsRcymwbmfjjIq\",\"p\":\"\",\"c\":[\"\",\"models\",\"\"],\"i\":false,\"f\":[[[\"\",{\"children\":[\"models\",{\"children\":[\"__PAGE__\",{}]}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/1d1f6bc532e5f43f.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[[\"$\",\"head\",null,{\"children\":[\"$\",\"link\",null,{\"rel\":\"icon\",\"type\":\"image/png\",\"href\":\"/favicon.png\"}]}],[\"$\",\"body\",null,{\"children\":[[\"$\",\"$L2\",null,{}],[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}],[\"$\",\"footer\",null,{\"children\":[\"$\",\"div\",null,{\"className\":\"container\",\"children\":[[\"$\",\"div\",null,{\"className\":\"footer-content\",\"children\":[[\"$\",\"div\",null,{\"className\":\"footer-section\",\"children\":[[\"$\",\"h4\",null,{\"children\":\"Zen LM\"}],[\"$\",\"p\",null,{\"children\":\"Open foundation models for agentic AI. 30+ models from 0.6B to 1T parameters.\"}]]}],[\"$\",\"div\",null,{\"className\":\"footer-section\",\"children\":[[\"$\",\"h4\",null,{\"children\":\"Zen Coder\"}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":[\"$\",\"$L5\",null,{\"href\":\"/models#zen-coder\",\"children\":\"Zen Coder 4B\"}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"$L5\",null,{\"href\":\"/models#zen-coder\",\"children\":\"Zen Coder 24B\"}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"$L5\",null,{\"href\":\"/models#zen-coder\",\"children\":\"Zen Coder 123B\"}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"$L5\",null,{\"href\":\"/models#zen-coder\",\"children\":\"Zen Coder Max (358B)\"}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"$L5\",null,{\"href\":\"/models#zen-coder\",\"children\":\"Zen Coder Ultra (1T)\"}]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"footer-section\",\"children\":[[\"$\",\"h4\",null,{\"children\":\"Model Family\"}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":[\"$\",\"$L5\",null,{\"href\":\"/models#core-models\",\"children\":\"zen-nano (0.6B)\"}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"$L5\",null,{\"href\":\"/models#core-models\",\"children\":\"zen-eco (4B)\"}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"$L5\",null,{\"href\":\"/models#core-models\",\"children\":\"zen-omni (7B)\"}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"$L5\",null,{\"href\":\"/models#multimodal\",\"children\":\"zen-vl (Vision)\"}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"$L5\",null,{\"href\":\"/models#multimodal\",\"children\":\"zen-3d (3D Gen)\"}]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"footer-section\",\"children\":[[\"$\",\"h4\",null,{\"children\":\"Resources\"}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":[\"$\",\"$L5\",null,{\"href\":\"/datasets\",\"children\":\"Training Data\"}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"a\",null,{\"href\":\"https://huggingface.co/zenlm\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":\"HuggingFace\"}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"a\",null,{\"href\":\"https://github.com/zenlm\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":\"GitHub\"}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"$L5\",null,{\"href\":\"/research\",\"children\":\"Research Papers\"}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"a\",null,{\"href\":\"https://github.com/zenlm/zen-trainer\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":\"zen-trainer\"}]}]]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"footer-bottom\",\"children\":[\"$\",\"p\",null,{\"children\":\"Â© 2025 Zen Authors. All rights reserved. Built with clarity and purpose.\"}]}]]}]}],[\"$\",\"script\",null,{\"src\":\"/assets/js/main.js\",\"async\":true}]]}]]}]]}],{\"children\":[\"models\",[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[\"__PAGE__\",[\"$\",\"$1\",\"c\",{\"children\":[[\"$\",\"main\",null,{\"children\":[[\"$\",\"section\",null,{\"className\":\"hero\",\"children\":[\"$\",\"div\",null,{\"className\":\"container\",\"children\":[\"$L6\",\"$L7\",\"$L8\"]}]}],\"$L9\",\"$La\"]}],null,\"$Lb\"]}],{},null,false]},null,false]},null,false],\"$Lc\",false]],\"m\":\"$undefined\",\"G\":[\"$d\",[]],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"1f:I[4431,[],\"OutletBoundary\"]\n21:I[5278,[],\"AsyncMetadataOutlet\"]\n23:I[4431,[],\"ViewportBoundary\"]\n25:I[4431,[],\"MetadataBoundary\"]\n26:\"$Sreact.suspense\"\n6:[\"$\",\"h2\",null,{\"className\":\"hero-title\",\"children\":\"Zen Model Family\"}]\n7:[\"$\",\"p\",null,{\"className\":\"hero-subtitle\",\"children\":\"24+ models spanning language, vision, audio, video, 3D, and specialized tasks\"}]\n8:[\"$\",\"p\",null,{\"className\":\"hero-description\",\"children\":\"Complete model collection from 0.6B to 1T+ parameters. From efficient edge deployment to powerful cloud inference, each model is optimized for specific use cases while maintaining the same high standards of performance, transparency, and open-source accessibility.\"}]\n"])</script><script>self.__next_f.push([1,"9:[\"$\",\"section\",null,{\"id\":\"core-models\",\"className\":\"models-section\",\"children\":[\"$\",\"div\",null,{\"className\":\"container\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"section-title\",\"children\":\"Core Language Models\"}],[\"$\",\"p\",null,{\"className\":\"section-subtitle\",\"children\":\"Foundational models from nano to next-gen\"}],[\"$\",\"div\",null,{\"className\":\"models-grid\",\"children\":[[\"$\",\"div\",null,{\"className\":\"model-card complete\",\"children\":[[\"$\",\"div\",null,{\"className\":\"model-header\",\"children\":[[\"$\",\"h3\",null,{\"children\":\"zen-nano\"}],[\"$\",\"span\",null,{\"className\":\"badge badge-complete\",\"children\":\"Available\"}]]}],[\"$\",\"div\",null,{\"className\":\"model-specs\",\"children\":[[\"$\",\"div\",\"0\",{\"className\":\"spec\",\"children\":[[\"$\",\"span\",null,{\"className\":\"spec-label\",\"children\":\"Parameters\"}],[\"$\",\"span\",null,{\"className\":\"spec-value\",\"children\":\"0.6B\"}]]}],[\"$\",\"div\",\"1\",{\"className\":\"spec\",\"children\":[[\"$\",\"span\",null,{\"className\":\"spec-label\",\"children\":\"Base\"}],[\"$\",\"span\",null,{\"className\":\"spec-value\",\"children\":\"Qwen3-0.6B\"}]]}],[\"$\",\"div\",\"2\",{\"className\":\"spec\",\"children\":[[\"$\",\"span\",null,{\"className\":\"spec-label\",\"children\":\"Context\"}],[\"$\",\"span\",null,{\"className\":\"spec-value\",\"children\":\"32K tokens\"}]]}],[\"$\",\"div\",\"3\",{\"className\":\"spec\",\"children\":[[\"$\",\"span\",null,{\"className\":\"spec-label\",\"children\":\"Architecture\"}],[\"$\",\"span\",null,{\"className\":\"spec-value\",\"children\":\"28 layers, GQA\"}]]}]]}],[\"$\",\"p\",null,{\"className\":\"model-description\",\"children\":\"Ultra-efficient model for edge deployment and embedded systems. Perfect for on-device AI applications with minimal resource requirements.\"}],[\"$\",\"div\",null,{\"className\":\"model-formats\",\"children\":[[\"$\",\"span\",\"0\",{\"className\":\"format-tag\",\"children\":\"SafeTensors\"}],[\"$\",\"span\",\"1\",{\"className\":\"format-tag\",\"children\":\"GGUF\"}],[\"$\",\"span\",\"2\",{\"className\":\"format-tag\",\"children\":\"MLX\"}]]}],[\"$\",\"div\",null,{\"className\":\"model-actions\",\"children\":[[\"$\",\"a\",null,{\"href\":\"https://huggingface.co/zenlm/zen-nano\",\"className\":\"btn btn-sm btn-primary\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":\"ðŸ¤— HF\"}],[\"$\",\"a\",null,{\"href\":\"https://github.com/zenlm/zen-nano\",\"className\":\"btn btn-sm btn-secondary\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":\"ðŸ“¦ GitHub\"}],[\"$\",\"a\",null,{\"href\":\"https://github.com/zenlm/zen-nano/tree/main/docs\",\"className\":\"btn btn-sm btn-secondary\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":\"ðŸ“– Docs\"}],[\"$\",\"a\",null,{\"href\":\"/papers/zen-nano_whitepaper.pdf\",\"className\":\"btn btn-sm btn-secondary\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":\"ðŸ“„ Paper\"}]]}]]}],[\"$\",\"div\",null,{\"className\":\"model-card complete\",\"children\":[[\"$\",\"div\",null,{\"className\":\"model-header\",\"children\":[[\"$\",\"h3\",null,{\"children\":\"zen-eco\"}],[\"$\",\"span\",null,{\"className\":\"badge badge-complete\",\"children\":\"Available\"}]]}],[\"$\",\"div\",null,{\"className\":\"model-specs\",\"children\":[[\"$\",\"div\",\"0\",{\"className\":\"spec\",\"children\":[[\"$\",\"span\",null,{\"className\":\"spec-label\",\"children\":\"Parameters\"}],[\"$\",\"span\",null,{\"className\":\"spec-value\",\"children\":\"4B\"}]]}],[\"$\",\"div\",\"1\",{\"className\":\"spec\",\"children\":[[\"$\",\"span\",null,{\"className\":\"spec-label\",\"children\":\"Base\"}],[\"$\",\"span\",null,{\"className\":\"spec-value\",\"children\":\"Qwen3-3B\"}]]}],[\"$\",\"div\",\"2\",{\"className\":\"spec\",\"children\":[[\"$\",\"span\",null,{\"className\":\"spec-label\",\"children\":\"Context\"}],[\"$\",\"span\",null,{\"className\":\"spec-value\",\"children\":\"32K tokens\"}]]}],[\"$\",\"div\",\"3\",{\"className\":\"spec\",\"children\":[[\"$\",\"span\",null,{\"className\":\"spec-label\",\"children\":\"Variants\"}],[\"$\",\"span\",null,{\"className\":\"spec-value\",\"children\":\"Instruct, Agent, Coder, Thinking\"}]]}]]}],[\"$\",\"p\",null,{\"className\":\"model-description\",\"children\":\"Balanced performance and efficiency for general-purpose applications. Multiple specialized variants for different use cases.\"}],[\"$\",\"div\",null,{\"className\":\"model-formats\",\"children\":[[\"$\",\"span\",\"0\",{\"className\":\"format-tag\",\"children\":\"SafeTensors\"}],[\"$\",\"span\",\"1\",{\"className\":\"format-tag\",\"children\":\"GGUF\"}],[\"$\",\"span\",\"2\",{\"className\":\"format-tag\",\"children\":\"MLX\"}]]}],[\"$\",\"div\",null,{\"className\":\"model-actions\",\"children\":[[\"$\",\"a\",null,{\"href\":\"https://huggingface.co/zenlm/zen-eco\",\"className\":\"btn btn-sm btn-primary\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":\"ðŸ¤— HF\"}],[\"$\",\"a\",null,{\"href\":\"https://github.com/zenlm/zen-eco\",\"className\":\"btn btn-sm btn-secondary\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":\"ðŸ“¦ GitHub\"}],\"$undefined\",\"$undefined\"]}]]}],[\"$\",\"div\",null,{\"className\":\"model-card complete\",\"children\":[[\"$\",\"div\",null,{\"className\":\"model-header\",\"children\":[\"$Le\",\"$Lf\"]}],\"$L10\",\"$L11\",\"$L12\",\"$L13\"]}],\"$L14\",\"$L15\"]}]]}]}]\n"])</script><script>self.__next_f.push([1,"a:[\"$\",\"section\",null,{\"id\":\"multimodal\",\"className\":\"models-section\",\"children\":[\"$\",\"div\",null,{\"className\":\"container\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"section-title\",\"children\":\"Multimodal Models\"}],[\"$\",\"p\",null,{\"className\":\"section-subtitle\",\"children\":\"Vision, Audio, Video, and 3D Generation\"}],[\"$\",\"div\",null,{\"className\":\"models-grid\",\"children\":[[\"$\",\"div\",null,{\"className\":\"model-card complete\",\"children\":[[\"$\",\"div\",null,{\"className\":\"model-header\",\"children\":[[\"$\",\"h3\",null,{\"children\":\"zen-vl\"}],[\"$\",\"span\",null,{\"className\":\"badge badge-complete\",\"children\":\"Available\"}]]}],[\"$\",\"div\",null,{\"className\":\"model-specs\",\"children\":[[\"$\",\"div\",\"0\",{\"className\":\"spec\",\"children\":[[\"$\",\"span\",null,{\"className\":\"spec-label\",\"children\":\"Type\"}],[\"$\",\"span\",null,{\"className\":\"spec-value\",\"children\":\"Vision-Language\"}]]}],[\"$\",\"div\",\"1\",{\"className\":\"spec\",\"children\":[[\"$\",\"span\",null,{\"className\":\"spec-label\",\"children\":\"Base\"}],[\"$\",\"span\",null,{\"className\":\"spec-value\",\"children\":\"Qwen3-VL\"}]]}],[\"$\",\"div\",\"2\",{\"className\":\"spec\",\"children\":[[\"$\",\"span\",null,{\"className\":\"spec-label\",\"children\":\"Sizes\"}],[\"$\",\"span\",null,{\"className\":\"spec-value\",\"children\":\"4B, 8B, 30B\"}]]}],[\"$\",\"div\",\"3\",{\"className\":\"spec\",\"children\":[[\"$\",\"span\",null,{\"className\":\"spec-label\",\"children\":\"Variants\"}],[\"$\",\"span\",null,{\"className\":\"spec-value\",\"children\":\"Instruct, Agent\"}]]}],[\"$\",\"div\",\"4\",{\"className\":\"spec\",\"children\":[[\"$\",\"span\",null,{\"className\":\"spec-label\",\"children\":\"Focus\"}],[\"$\",\"span\",null,{\"className\":\"spec-value\",\"children\":\"Function Calling\"}]]}]]}],[\"$\",\"p\",null,{\"className\":\"model-description\",\"children\":\"Next-generation vision-language model with advanced function calling capabilities. Trained on Agent Data Protocol (ADP) and xLAM datasets for superior agent performance and tool use.\"}],[\"$\",\"div\",null,{\"className\":\"model-formats\",\"children\":[[\"$\",\"span\",\"0\",{\"className\":\"format-tag\",\"children\":\"SafeTensors\"}],[\"$\",\"span\",\"1\",{\"className\":\"format-tag\",\"children\":\"GGUF\"}]]}],[\"$\",\"div\",null,{\"className\":\"model-actions\",\"children\":[[\"$\",\"a\",null,{\"href\":\"https://huggingface.co/zenlm/zen-vl-4b-instruct\",\"className\":\"btn btn-sm btn-primary\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":\"ðŸ¤— HF\"}],[\"$\",\"a\",null,{\"href\":\"https://github.com/zenlm/zen-vl\",\"className\":\"btn btn-sm btn-secondary\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":\"ðŸ“¦ GitHub\"}],\"$undefined\",\"$undefined\"]}]]}],[\"$\",\"div\",null,{\"className\":\"model-card complete\",\"children\":[[\"$\",\"div\",null,{\"className\":\"model-header\",\"children\":[[\"$\",\"h3\",null,{\"children\":\"zen-designer\"}],[\"$\",\"span\",null,{\"className\":\"badge badge-complete\",\"children\":\"Available\"}]]}],[\"$\",\"div\",null,{\"className\":\"model-specs\",\"children\":[[\"$\",\"div\",\"0\",{\"className\":\"spec\",\"children\":[[\"$\",\"span\",null,{\"className\":\"spec-label\",\"children\":\"Type\"}],[\"$\",\"span\",null,{\"className\":\"spec-value\",\"children\":\"Vision-Language\"}]]}],[\"$\",\"div\",\"1\",{\"className\":\"spec\",\"children\":[[\"$\",\"span\",null,{\"className\":\"spec-label\",\"children\":\"Base\"}],[\"$\",\"span\",null,{\"className\":\"spec-value\",\"children\":\"Qwen-VL\"}]]}],[\"$\",\"div\",\"2\",{\"className\":\"spec\",\"children\":[[\"$\",\"span\",null,{\"className\":\"spec-label\",\"children\":\"Variants\"}],[\"$\",\"span\",null,{\"className\":\"spec-value\",\"children\":\"Instruct, Thinking\"}]]}],[\"$\",\"div\",\"3\",{\"className\":\"spec\",\"children\":[[\"$\",\"span\",null,{\"className\":\"spec-label\",\"children\":\"Focus\"}],[\"$\",\"span\",null,{\"className\":\"spec-value\",\"children\":\"Visual Understanding\"}]]}]]}],[\"$\",\"p\",null,{\"className\":\"model-description\",\"children\":\"Advanced vision-language model for image understanding, analysis, and reasoning. Supports visual question answering, OCR, and detailed scene description.\"}],[\"$\",\"div\",null,{\"className\":\"model-formats\",\"children\":[[\"$\",\"span\",\"0\",{\"className\":\"format-tag\",\"children\":\"SafeTensors\"}]]}],[\"$\",\"div\",null,{\"className\":\"model-actions\",\"children\":[\"$undefined\",[\"$\",\"a\",null,{\"href\":\"https://github.com/zenlm/zen-designer\",\"className\":\"btn btn-sm btn-secondary\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":\"ðŸ“¦ GitHub\"}],\"$undefined\",\"$undefined\"]}]]}],[\"$\",\"div\",null,{\"className\":\"model-card complete\",\"children\":[[\"$\",\"div\",null,{\"className\":\"model-header\",\"children\":[[\"$\",\"h3\",null,{\"children\":\"zen-artist\"}],[\"$\",\"span\",null,{\"className\":\"badge badge-complete\",\"children\":\"Available\"}]]}],[\"$\",\"div\",null,{\"className\":\"model-specs\",\"children\":[[\"$\",\"div\",\"0\",{\"className\":\"spec\",\"children\":[[\"$\",\"span\",null,{\"className\":\"spec-label\",\"children\":\"Type\"}],[\"$\",\"span\",null,{\"className\":\"spec-value\",\"children\":\"Text-to-Image\"}]]}],[\"$\",\"div\",\"1\",{\"className\":\"spec\",\"children\":[[\"$\",\"span\",null,{\"className\":\"spec-label\",\"children\":\"Base\"}],\"$L16\"]}],\"$L17\",\"$L18\"]}],\"$L19\",\"$L1a\",\"$L1b\"]}],\"$L1c\",\"$L1d\",\"$L1e\"]}]]}]}]\n"])</script><script>self.__next_f.push([1,"b:[\"$\",\"$L1f\",null,{\"children\":[\"$L20\",[\"$\",\"$L21\",null,{\"promise\":\"$@22\"}]]}]\nc:[\"$\",\"$1\",\"h\",{\"children\":[null,[[\"$\",\"$L23\",null,{\"children\":\"$L24\"}],null],[\"$\",\"$L25\",null,{\"children\":[\"$\",\"div\",null,{\"hidden\":true,\"children\":[\"$\",\"$26\",null,{\"fallback\":null,\"children\":\"$L27\"}]}]}]]}]\n"])</script><script>self.__next_f.push([1,"e:[\"$\",\"h3\",null,{\"children\":\"zen-omni\"}]\nf:[\"$\",\"span\",null,{\"className\":\"badge badge-complete\",\"children\":\"Available\"}]\n"])</script><script>self.__next_f.push([1,"10:[\"$\",\"div\",null,{\"className\":\"model-specs\",\"children\":[[\"$\",\"div\",\"0\",{\"className\":\"spec\",\"children\":[[\"$\",\"span\",null,{\"className\":\"spec-label\",\"children\":\"Parameters\"}],[\"$\",\"span\",null,{\"className\":\"spec-value\",\"children\":\"7B\"}]]}],[\"$\",\"div\",\"1\",{\"className\":\"spec\",\"children\":[[\"$\",\"span\",null,{\"className\":\"spec-label\",\"children\":\"Base\"}],[\"$\",\"span\",null,{\"className\":\"spec-value\",\"children\":\"Qwen3-Omni\"}]]}],[\"$\",\"div\",\"2\",{\"className\":\"spec\",\"children\":[[\"$\",\"span\",null,{\"className\":\"spec-label\",\"children\":\"Modalities\"}],[\"$\",\"span\",null,{\"className\":\"spec-value\",\"children\":\"Text + Vision + Audio\"}]]}],[\"$\",\"div\",\"3\",{\"className\":\"spec\",\"children\":[[\"$\",\"span\",null,{\"className\":\"spec-label\",\"children\":\"Type\"}],[\"$\",\"span\",null,{\"className\":\"spec-value\",\"children\":\"Multimodal\"}]]}]]}]\n"])</script><script>self.__next_f.push([1,"11:[\"$\",\"p\",null,{\"className\":\"model-description\",\"children\":\"Multimodal model based on Qwen3-Omni supporting text, vision, and audio understanding simultaneously. NOT Qwen2.5!\"}]\n12:[\"$\",\"div\",null,{\"className\":\"model-formats\",\"children\":[[\"$\",\"span\",\"0\",{\"className\":\"format-tag\",\"children\":\"SafeTensors\"}]]}]\n"])</script><script>self.__next_f.push([1,"13:[\"$\",\"div\",null,{\"className\":\"model-actions\",\"children\":[[\"$\",\"a\",null,{\"href\":\"https://huggingface.co/zenlm/zen-omni\",\"className\":\"btn btn-sm btn-primary\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":\"ðŸ¤— HF\"}],[\"$\",\"a\",null,{\"href\":\"https://github.com/zenlm/zen-omni\",\"className\":\"btn btn-sm btn-secondary\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":\"ðŸ“¦ GitHub\"}],[\"$\",\"a\",null,{\"href\":\"https://github.com/zenlm/zen-omni/tree/main/docs\",\"className\":\"btn btn-sm btn-secondary\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":\"ðŸ“– Docs\"}],[\"$\",\"a\",null,{\"href\":\"/papers/zen-omni_whitepaper.pdf\",\"className\":\"btn btn-sm btn-secondary\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":\"ðŸ“„ Paper\"}]]}]\n"])</script><script>self.__next_f.push([1,"14:[\"$\",\"div\",null,{\"className\":\"model-card complete\",\"children\":[[\"$\",\"div\",null,{\"className\":\"model-header\",\"children\":[[\"$\",\"h3\",null,{\"children\":\"zen-coder\"}],[\"$\",\"span\",null,{\"className\":\"badge badge-complete\",\"children\":\"Available\"}]]}],[\"$\",\"div\",null,{\"className\":\"model-specs\",\"children\":[[\"$\",\"div\",\"0\",{\"className\":\"spec\",\"children\":[[\"$\",\"span\",null,{\"className\":\"spec-label\",\"children\":\"Parameters\"}],[\"$\",\"span\",null,{\"className\":\"spec-value\",\"children\":\"14B\"}]]}],[\"$\",\"div\",\"1\",{\"className\":\"spec\",\"children\":[[\"$\",\"span\",null,{\"className\":\"spec-label\",\"children\":\"Base\"}],[\"$\",\"span\",null,{\"className\":\"spec-value\",\"children\":\"Qwen3-Coder-14B\"}]]}],[\"$\",\"div\",\"2\",{\"className\":\"spec\",\"children\":[[\"$\",\"span\",null,{\"className\":\"spec-label\",\"children\":\"Context\"}],[\"$\",\"span\",null,{\"className\":\"spec-value\",\"children\":\"128K tokens\"}]]}],[\"$\",\"div\",\"3\",{\"className\":\"spec\",\"children\":[[\"$\",\"span\",null,{\"className\":\"spec-label\",\"children\":\"Focus\"}],[\"$\",\"span\",null,{\"className\":\"spec-value\",\"children\":\"Code Generation\"}]]}]]}],[\"$\",\"p\",null,{\"className\":\"model-description\",\"children\":\"Specialized for code generation, debugging, and software engineering tasks. Supports 100+ programming languages with extended context.\"}],[\"$\",\"div\",null,{\"className\":\"model-formats\",\"children\":[[\"$\",\"span\",\"0\",{\"className\":\"format-tag\",\"children\":\"SafeTensors\"}],[\"$\",\"span\",\"1\",{\"className\":\"format-tag\",\"children\":\"GGUF\"}],[\"$\",\"span\",\"2\",{\"className\":\"format-tag\",\"children\":\"MLX\"}]]}],[\"$\",\"div\",null,{\"className\":\"model-actions\",\"children\":[[\"$\",\"a\",null,{\"href\":\"https://huggingface.co/zenlm/zen-coder\",\"className\":\"btn btn-sm btn-primary\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":\"ðŸ¤— HF\"}],[\"$\",\"a\",null,{\"href\":\"https://github.com/zenlm/zen-coder\",\"className\":\"btn btn-sm btn-secondary\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":\"ðŸ“¦ GitHub\"}],\"$undefined\",\"$undefined\"]}]]}]\n"])</script><script>self.__next_f.push([1,"15:[\"$\",\"div\",null,{\"className\":\"model-card complete\",\"children\":[[\"$\",\"div\",null,{\"className\":\"model-header\",\"children\":[[\"$\",\"h3\",null,{\"children\":\"zen-next\"}],[\"$\",\"span\",null,{\"className\":\"badge badge-complete\",\"children\":\"Available\"}]]}],[\"$\",\"div\",null,{\"className\":\"model-specs\",\"children\":[[\"$\",\"div\",\"0\",{\"className\":\"spec\",\"children\":[[\"$\",\"span\",null,{\"className\":\"spec-label\",\"children\":\"Parameters\"}],[\"$\",\"span\",null,{\"className\":\"spec-value\",\"children\":\"32B\"}]]}],[\"$\",\"div\",\"1\",{\"className\":\"spec\",\"children\":[[\"$\",\"span\",null,{\"className\":\"spec-label\",\"children\":\"Base\"}],[\"$\",\"span\",null,{\"className\":\"spec-value\",\"children\":\"Qwen3-32B\"}]]}],[\"$\",\"div\",\"2\",{\"className\":\"spec\",\"children\":[[\"$\",\"span\",null,{\"className\":\"spec-label\",\"children\":\"Context\"}],[\"$\",\"span\",null,{\"className\":\"spec-value\",\"children\":\"32K tokens\"}]]}],[\"$\",\"div\",\"3\",{\"className\":\"spec\",\"children\":[[\"$\",\"span\",null,{\"className\":\"spec-label\",\"children\":\"Focus\"}],[\"$\",\"span\",null,{\"className\":\"spec-value\",\"children\":\"Frontier\"}]]}]]}],[\"$\",\"p\",null,{\"className\":\"model-description\",\"children\":\"Our flagship model pushing the boundaries of performance and capability. For the most demanding applications requiring maximum intelligence.\"}],[\"$\",\"div\",null,{\"className\":\"model-formats\",\"children\":[[\"$\",\"span\",\"0\",{\"className\":\"format-tag\",\"children\":\"SafeTensors\"}],[\"$\",\"span\",\"1\",{\"className\":\"format-tag\",\"children\":\"GGUF\"}]]}],[\"$\",\"div\",null,{\"className\":\"model-actions\",\"children\":[\"$undefined\",[\"$\",\"a\",null,{\"href\":\"https://github.com/zenlm/zen-next\",\"className\":\"btn btn-sm btn-secondary\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":\"ðŸ“¦ GitHub\"}],\"$undefined\",\"$undefined\"]}]]}]\n"])</script><script>self.__next_f.push([1,"16:[\"$\",\"span\",null,{\"className\":\"spec-value\",\"children\":\"Qwen-Image\"}]\n17:[\"$\",\"div\",\"2\",{\"className\":\"spec\",\"children\":[[\"$\",\"span\",null,{\"className\":\"spec-label\",\"children\":\"Variants\"}],[\"$\",\"span\",null,{\"className\":\"spec-value\",\"children\":\"Base, Edit\"}]]}]\n18:[\"$\",\"div\",\"3\",{\"className\":\"spec\",\"children\":[[\"$\",\"span\",null,{\"className\":\"spec-label\",\"children\":\"Focus\"}],[\"$\",\"span\",null,{\"className\":\"spec-value\",\"children\":\"Image Generation\"}]]}]\n19:[\"$\",\"p\",null,{\"className\":\"model-description\",\"children\":\"High-quality image generation from text descriptions. zen-artist-edit provides advanced image editing capabilities with natural language instructions.\"}]\n1a:[\"$\",\"div\",null,{\"className\":\"model-formats\",\"children\":[[\"$\",\"span\",\"0\",{\"className\":\"format-tag\",\"children\":\"SafeTensors\"}],[\"$\",\"span\",\"1\",{\"className\":\"format-tag\",\"children\":\"Diffusers\"}]]}]\n1b:[\"$\",\"div\",null,{\"className\":\"model-actions\",\"children\":[\"$undefined\",[\"$\",\"a\",null,{\"href\":\"https://github.com/zenlm/zen-artist\",\"className\":\"btn btn-sm btn-secondary\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":\"ðŸ“¦ GitHub\"}],\"$undefined\",\"$undefined\"]}]\n"])</script><script>self.__next_f.push([1,"1c:[\"$\",\"div\",null,{\"className\":\"model-card complete\",\"children\":[[\"$\",\"div\",null,{\"className\":\"model-header\",\"children\":[[\"$\",\"h3\",null,{\"children\":\"zen-video\"}],[\"$\",\"span\",null,{\"className\":\"badge badge-complete\",\"children\":\"Available\"}]]}],[\"$\",\"div\",null,{\"className\":\"model-specs\",\"children\":[[\"$\",\"div\",\"0\",{\"className\":\"spec\",\"children\":[[\"$\",\"span\",null,{\"className\":\"spec-label\",\"children\":\"Type\"}],[\"$\",\"span\",null,{\"className\":\"spec-value\",\"children\":\"Text-to-Video\"}]]}],[\"$\",\"div\",\"1\",{\"className\":\"spec\",\"children\":[[\"$\",\"span\",null,{\"className\":\"spec-label\",\"children\":\"Base\"}],[\"$\",\"span\",null,{\"className\":\"spec-value\",\"children\":\"HunyuanVideo\"}]]}],[\"$\",\"div\",\"2\",{\"className\":\"spec\",\"children\":[[\"$\",\"span\",null,{\"className\":\"spec-label\",\"children\":\"Variants\"}],[\"$\",\"span\",null,{\"className\":\"spec-value\",\"children\":\"T2V, I2V\"}]]}],[\"$\",\"div\",\"3\",{\"className\":\"spec\",\"children\":[[\"$\",\"span\",null,{\"className\":\"spec-label\",\"children\":\"Focus\"}],[\"$\",\"span\",null,{\"className\":\"spec-value\",\"children\":\"Video Generation\"}]]}]]}],[\"$\",\"p\",null,{\"className\":\"model-description\",\"children\":\"State-of-the-art video generation from text descriptions. zen-video-i2v provides image-to-video generation with fine control over motion and dynamics.\"}],[\"$\",\"div\",null,{\"className\":\"model-formats\",\"children\":[[\"$\",\"span\",\"0\",{\"className\":\"format-tag\",\"children\":\"SafeTensors\"}]]}],[\"$\",\"div\",null,{\"className\":\"model-actions\",\"children\":[\"$undefined\",[\"$\",\"a\",null,{\"href\":\"https://github.com/zenlm/zen-video\",\"className\":\"btn btn-sm btn-secondary\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":\"ðŸ“¦ GitHub\"}],\"$undefined\",\"$undefined\"]}]]}]\n"])</script><script>self.__next_f.push([1,"1d:[\"$\",\"div\",null,{\"className\":\"model-card complete\",\"children\":[[\"$\",\"div\",null,{\"className\":\"model-header\",\"children\":[[\"$\",\"h3\",null,{\"children\":\"zen-3d\"}],[\"$\",\"span\",null,{\"className\":\"badge badge-complete\",\"children\":\"Available\"}]]}],[\"$\",\"div\",null,{\"className\":\"model-specs\",\"children\":[[\"$\",\"div\",\"0\",{\"className\":\"spec\",\"children\":[[\"$\",\"span\",null,{\"className\":\"spec-label\",\"children\":\"Type\"}],[\"$\",\"span\",null,{\"className\":\"spec-value\",\"children\":\"3D Generation\"}]]}],[\"$\",\"div\",\"1\",{\"className\":\"spec\",\"children\":[[\"$\",\"span\",null,{\"className\":\"spec-label\",\"children\":\"Input\"}],[\"$\",\"span\",null,{\"className\":\"spec-value\",\"children\":\"Text, Image, Point Cloud\"}]]}],[\"$\",\"div\",\"2\",{\"className\":\"spec\",\"children\":[[\"$\",\"span\",null,{\"className\":\"spec-label\",\"children\":\"Output\"}],[\"$\",\"span\",null,{\"className\":\"spec-value\",\"children\":\"3D Meshes\"}]]}],[\"$\",\"div\",\"3\",{\"className\":\"spec\",\"children\":[[\"$\",\"span\",null,{\"className\":\"spec-label\",\"children\":\"Focus\"}],[\"$\",\"span\",null,{\"className\":\"spec-value\",\"children\":\"3D Assets\"}]]}]]}],[\"$\",\"p\",null,{\"className\":\"model-description\",\"children\":\"Generate high-quality 3D models from various input modalities. Perfect for game development, AR/VR, and 3D content creation.\"}],[\"$\",\"div\",null,{\"className\":\"model-formats\",\"children\":[[\"$\",\"span\",\"0\",{\"className\":\"format-tag\",\"children\":\"SafeTensors\"}]]}],[\"$\",\"div\",null,{\"className\":\"model-actions\",\"children\":[\"$undefined\",[\"$\",\"a\",null,{\"href\":\"https://github.com/zenlm/zen-3d\",\"className\":\"btn btn-sm btn-secondary\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":\"ðŸ“¦ GitHub\"}],\"$undefined\",\"$undefined\"]}]]}]\n"])</script><script>self.__next_f.push([1,"1e:[\"$\",\"div\",null,{\"className\":\"model-card complete\",\"children\":[[\"$\",\"div\",null,{\"className\":\"model-header\",\"children\":[[\"$\",\"h3\",null,{\"children\":\"zen-musician\"}],[\"$\",\"span\",null,{\"className\":\"badge badge-complete\",\"children\":\"Available\"}]]}],[\"$\",\"div\",null,{\"className\":\"model-specs\",\"children\":[[\"$\",\"div\",\"0\",{\"className\":\"spec\",\"children\":[[\"$\",\"span\",null,{\"className\":\"spec-label\",\"children\":\"Type\"}],[\"$\",\"span\",null,{\"className\":\"spec-value\",\"children\":\"Music Generation\"}]]}],[\"$\",\"div\",\"1\",{\"className\":\"spec\",\"children\":[[\"$\",\"span\",null,{\"className\":\"spec-label\",\"children\":\"Input\"}],[\"$\",\"span\",null,{\"className\":\"spec-value\",\"children\":\"Text, Audio\"}]]}],[\"$\",\"div\",\"2\",{\"className\":\"spec\",\"children\":[[\"$\",\"span\",null,{\"className\":\"spec-label\",\"children\":\"Output\"}],[\"$\",\"span\",null,{\"className\":\"spec-value\",\"children\":\"Music, Audio\"}]]}],[\"$\",\"div\",\"3\",{\"className\":\"spec\",\"children\":[[\"$\",\"span\",null,{\"className\":\"spec-label\",\"children\":\"Focus\"}],[\"$\",\"span\",null,{\"className\":\"spec-value\",\"children\":\"Music Creation\"}]]}]]}],[\"$\",\"p\",null,{\"className\":\"model-description\",\"children\":\"Generate high-quality music from text descriptions or audio samples. Supports multiple genres, instruments, and musical styles.\"}],[\"$\",\"div\",null,{\"className\":\"model-formats\",\"children\":[[\"$\",\"span\",\"0\",{\"className\":\"format-tag\",\"children\":\"SafeTensors\"}]]}],[\"$\",\"div\",null,{\"className\":\"model-actions\",\"children\":[\"$undefined\",[\"$\",\"a\",null,{\"href\":\"https://github.com/zenlm/zen-musician\",\"className\":\"btn btn-sm btn-secondary\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":\"ðŸ“¦ GitHub\"}],\"$undefined\",\"$undefined\"]}]]}]\n"])</script><script>self.__next_f.push([1,"24:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n20:null\n"])</script><script>self.__next_f.push([1,"22:{\"metadata\":[[\"$\",\"title\",\"0\",{\"children\":\"Zen LM Models - Complete Model Family\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"Complete collection of Zen Language Models - from nano to next-gen, language to multimodal\"}],[\"$\",\"meta\",\"2\",{\"name\":\"keywords\",\"content\":\"AI, LLM, Agentic AI, Code Generation, Zen Coder, Multimodal, Open Source, Machine Learning\"}]],\"error\":null,\"digest\":\"$undefined\"}\n"])</script><script>self.__next_f.push([1,"27:\"$22:metadata\"\n"])</script></body></html>