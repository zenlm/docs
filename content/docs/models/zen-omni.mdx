---
title: zen-omni
description: 7B multimodal model for text, image, and audio
---

# zen-omni

**Multimodal Understanding**

<Cards>
  <Card title="HuggingFace" href="https://huggingface.co/zenlm/zen-omni" />
</Cards>

## Overview

| Attribute | Value |
|-----------|-------|
| **Parameters** | ~7B |
| **Context Length** | 32K tokens |
| **Base Model** | Qwen3-Omni (NOT Qwen2.5!) |
| **Modalities** | Text, Image, Audio/Speech |
| **License** | Apache 2.0 |

## Capabilities

- **Text generation** and understanding
- **Image analysis** and visual reasoning
- **Audio/Speech** processing
- **Cross-modal** reasoning

## Usage

```python
from transformers import AutoModelForCausalLM, AutoProcessor
import torch

model = AutoModelForCausalLM.from_pretrained(
    "zenlm/zen-omni",
    torch_dtype=torch.bfloat16,
    device_map="auto",
)
processor = AutoProcessor.from_pretrained("zenlm/zen-omni")
```

<Callout type="warning">
  zen-omni is based on Qwen3-Omni, NOT Qwen2.5. All Zen models use Qwen3+ architecture.
</Callout>
